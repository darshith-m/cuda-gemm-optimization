{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **General Matrix Multiplication (GEMM) Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-requistes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code adds the CUDA 12.6 compiler's location to the system's PATH environment variable and then displays the version information for NVIDIA's CUDA compiler (nvcc), which is used for GPU programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda-12.6/bin\" # Add your path to CUDA\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Device details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  7 02:53:26 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.59                 Driver Version: 556.13         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8             12W /   95W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function to execute object files repeatedly and print the average kernel runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import statistics\n",
    "import time  # Added for sleep functionality\n",
    "\n",
    "def run_gemm(executable_path, choice, m, n, k, runs=10, sleep_time=2):  # Added sleep_time parameter\n",
    "    times = []\n",
    "    \n",
    "    for i in range(runs):\n",
    "        result = subprocess.run([executable_path, str(choice), str(m), str(n), str(k)],\n",
    "                              capture_output=True, text=True)\n",
    "        print(f\"Output {i+1} - {result.stdout}\") \n",
    "        match = re.search(r'CUDA kernel time: (\\d+\\.\\d+)', result.stdout)\n",
    "        if match:\n",
    "            cuda_time = float(match.group(1))\n",
    "            times.append(cuda_time)\n",
    "        else:\n",
    "            print(f\"Warning: No time found in output: {result.stdout}\")\n",
    "        \n",
    "        # Add sleep between runs, except for the last run\n",
    "        if i < runs - 1:\n",
    "            time.sleep(sleep_time)\n",
    "            print(f\"Sleeping for {sleep_time} seconds...\")\n",
    "    \n",
    "    # Calculate and print statistics\n",
    "    mean_time = statistics.mean(times)\n",
    "    std_dev = statistics.stdev(times) if len(times) > 1 else 0\n",
    "    min_time = min(times) if times else 0\n",
    "    max_time = max(times) if times else 0\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Mean: {mean_time:.2f} ms\")\n",
    "    print(f\"Std Dev: {std_dev:.2f} ms\")\n",
    "    print(f\"Min: {min_time:.2f} ms\")\n",
    "    print(f\"Max: {max_time:.2f} ms\")\n",
    "    \n",
    "    return mean_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 - Naive GEMM Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_kernel_mul.png\" alt=\"Naive GEMM Multiplication\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows a naive GEMM (General Matrix Multiplication) kernel implementation using threads. Each thread accesses matrix elements based on its ID: `x = blockDim.x * blockIdx.x + threadIdx.x` and `y = blockDim.y * blockIdx.y + threadIdx.y`. Within the B matrix, threads in a warp access the same values (broadcast), while in the A matrix, threads access non-consecutive memory locations (non-coalesced memory access), which is inefficient. The C matrix shows how different threads (0,0), (0,1), (0,2) etc., compute their respective output elements through these memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_kernel_memory_access.png\" alt=\"Naive Kernel Memory Access\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates a memory access pattern issue in GPU computing. It shows two warps (groups of threads) accessing memory in a non-coalesced pattern, meaning threads access scattered memory locations rather than consecutive ones. Each warp requires 4x32B loads (8 loads total), which is inefficient. The crossing lines between thread indices and memory locations visualize this scattered access pattern. This non-optimal memory access results in performance penalties because too many separate load operations are needed to execute each warp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/01_naive_gemm.cu](./src/01_naive_gemm.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc -lineinfo -g -o ./src/01_naive_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1226.8019 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1237.8530 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1254.0999 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1255.6365 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1215.9648 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1230.0734 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1246.9884 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1255.2172 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1256.9995 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1220.3922 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 1240.00 ms\n",
      "Std Dev: 15.82 ms\n",
      "Min: 1215.96 ms\n",
      "Max: 1257.00 ms\n",
      "Average Naive GEMM time: 1240.00268\n"
     ]
    }
   ],
   "source": [
    "naive_gemm_time = run_gemm(\"./src/01_naive_gemm\", 1, 4096, 4096, 4096)\n",
    "print(f\"Average Naive GEMM time: {naive_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 31766 (/home/darshith/code/cuda-gemm-optimization/src/01_naive_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "Naive GEMM Kernel:\n",
      "==PROF== Profiling \"gemmNaive\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "CUDA kernel time: 190328.3906 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 31766\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/01_naive_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "! ncu -f --set full -o ./profiles/01_naive_gemm ./src/01_naive_gemm 1 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_sol.png\" alt=\"Naive SOL\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU throughput graph shows significant memory utilization (approximately 90% of SOL) while compute utilization is relatively low (around 15% of SOL). This indicates the naive GEMM implementation is heavily memory-bound, primarily constrained by memory bandwidth rather than computational capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_roofline.png\" alt=\"Naive Roofline\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The roofline plot reveals this naive GEMM implementation is significantly underperforming compared to the hardware's theoretical peak performance, achieving around 51.8 TFLOP/s with an arithmetic intensity of 14.58 FLOP/byte. The large gap between the actual performance point and the peak performance line suggests substantial room for optimization through techniques like better memory access patterns, cache utilization, and vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 - Coalesced Memory GEMM Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/coalesced_memory_mul.png\" alt=\"Coalesced Memory Multiplication\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows an optimized memory coalesced GEMM (General Matrix Multiplication) kernel design. Unlike the naive version, threads access consecutive memory locations in matrix B, enabling memory coalescing and better performance. For matrix A, all threads within a warp access the same values (broadcast). The coordinates are calculated as: `x = blockIdx.x * BLOCK_SIZE + (threadIdx.x / BLOCK_SIZE)` for matrix A's row access, and `y = blockIdx.y * BLOCK_SIZE + (threadIdx.y % BLOCK_SIZE)` for matrix B's column access. Threads (0,0), (0,1), and (0,2) are grouped in the same warp to optimize memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/coalesced_memory_access.png\" alt=\"Coalesced Memory Access\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows an optimized memory coalesced access pattern where threads within each warp (Warp-0 and Warp-1) access consecutive memory locations. Each warp now requires only 2x32B loads (4 loads total), half of what was needed in the non-coalesced version. The straight vertical lines from thread indices to memory locations indicate efficient coalesced memory access, improving performance by reducing the number of required load operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/02_memory_coalesced_gemm.cu](./src/02_memory_coalesced_gemm.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lineinfo -g -o ./src/02_memory_coalesced_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Global Memory Coalescing:\n",
      "CUDA kernel time: 245.2312 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Global Memory Coalescing:\n",
      "CUDA kernel time: 242.1448 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Global Memory Coalescing:\n",
      "CUDA kernel time: 243.8303 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Global Memory Coalescing:\n",
      "CUDA kernel time: 248.2520 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Global Memory Coalescing:\n",
      "CUDA kernel time: 239.8353 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Global Memory Coalescing:\n",
      "CUDA kernel time: 246.4755 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Global Memory Coalescing:\n",
      "CUDA kernel time: 245.2530 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Global Memory Coalescing:\n",
      "CUDA kernel time: 240.7190 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Global Memory Coalescing:\n",
      "CUDA kernel time: 245.7898 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Global Memory Coalescing:\n",
      "CUDA kernel time: 251.1602 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 244.87 ms\n",
      "Std Dev: 3.43 ms\n",
      "Min: 239.84 ms\n",
      "Max: 251.16 ms\n",
      "Average Global Memory Coalesced GEMM time: 244.86911\n"
     ]
    }
   ],
   "source": [
    "memory_coalesced_gemm_time = run_gemm(\"./src/02_memory_coalesced_gemm\", 2, 4096, 4096, 4096)\n",
    "print(f\"Average Global Memory Coalesced GEMM time: {memory_coalesced_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 32591 (/home/darshith/code/cuda-gemm-optimization/src/02_memory_coalesced_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "Global Memory Coalescing:\n",
      "==PROF== Profiling \"gemmMemCoalesced\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "CUDA kernel time: 104711.6641 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 32591\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/02_memory_coalesced_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/02_memory_coalesced_gemm ./src/02_memory_coalesced_gemm 2 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/gmem_coalesce_sol.png\" alt=\"Global Memory Coalesce SOL\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the naive version's heavily memory-bound performance (~90% memory, ~15% compute SOL), this optimized implementation shows balanced utilization with both compute and memory throughput reaching around 85% SOL. This balance, achieved through memory coalescing, indicates more efficient use of both computational and memory resources on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/gmem_coalesce_roofline.png\" alt=\"Global Memory Coalesce Roofline\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the naive implementation (51.8 GFLOP/s), this optimized version achieves 374.9 GFLOP/s with a slightly higher arithmetic intensity of 15.14 FLOP/byte, demonstrating a 7.2x speedup primarily through global memory coalescing, where adjacent threads access contiguous memory addresses to combine memory transactions into fewer, larger operations, significantly reducing memory latency and increasing effective bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 -  Shared Memory Cache-Blocking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/shared_memory_cache_blocking.png\" alt=\"Shared Memory Cache Blocking\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates a block-based matrix multiplication algorithm with a block size of 32. When multiplying matrices A and C, each matrix is divided into blocks of size BLOCK_SIZE (32). The starting addresses of blocks are calculated using formulas: `&A = row * BLOCK_SIZE * K` for matrix A's rows, `&B = col * BLOCK_SIZE` for B's columns, and `&C = (row * BLOCK_SIZE * K) + (col * BLOCK_SIZE)` for matrix C's position. As the algorithm processes each block, it increments A by BLOCK_SIZE within the same row, B by BLOCK_SIZE * N to move to the next block, and C moves to process the next row of blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/03_shared_memory_gemm.cu](./src/03_shared_memory_gemm.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lineinfo -g -o ./src/03_shared_memory_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 161.3495 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 174.2664 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 177.5903 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 178.5169 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 175.0734 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 164.2657 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 163.7582 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 178.3736 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 169.3051 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 177.4289 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 171.99 ms\n",
      "Std Dev: 6.72 ms\n",
      "Min: 161.35 ms\n",
      "Max: 178.52 ms\n",
      "Average Shared Memory GEMM time: 171.9928\n"
     ]
    }
   ],
   "source": [
    "shared_memory_gemm_time = run_gemm(\"./src/03_shared_memory_gemm\", 3, 4096, 4096, 4096)\n",
    "print(f\"Average Shared Memory GEMM time: {shared_memory_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 33179 (/home/darshith/code/cuda-gemm-optimization/src/03_shared_memory_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "Shared Memory Cache-Blocking:\n",
      "==PROF== Profiling \"gemmSharedMem\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "CUDA kernel time: 48190.5156 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 33179\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/03_shared_memory_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/03_shared_memory_gemm ./src/03_shared_memory_gemm 3 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/shared_mem_sol.png\" alt=\"Shared Memory SOL\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shared memory version shows slightly lower GPU throughput (~80% SOL) compared to the coalesced version (~85% SOL), but still maintains balanced compute and memory utilization. It leverages faster shared memory access to reduce global memory traffic, though the overhead of shared memory operations may explain the small performance difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/shared_mem_roofline.png\" alt=\"Shared Memory Roofline\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shared memory implementation reaches 505.0 GFLOP/s compared to 374.9 GFLOP/s in the previous version, with a slightly higher arithmetic intensity (15.85 vs 15.14 FLOP/byte). The 130.1 GFLOP/s improvement comes from optimized tile sizes and shared memory access patterns while maintaining ~80% SOL utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 - 1D Block-Tiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_block_tiling.png\" alt=\"1D Block tiling\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `TM` in the kernel specifies the number of rows in the output tile of \\( C \\) that each thread computes. Instead of handling a single element, each thread processes \\( TM \\) rows for its assigned column, storing partial results in local registers (`threadResult[TM]`). This approach increases the amount of computation per thread, reducing idle cycles and improving efficiency. By reusing the same tiles of \\( A \\) and \\( B \\) loaded into shared memory to compute all \\( TM \\) rows, the kernel minimizes global memory traffic, enhancing memory locality. This design choice significantly boosts arithmetic intensity by performing more computations relative to the data moved from global memory, leading to better utilization of GPU resources and improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/04_1d_block_tiling.cu](./src/04_1d_block_tiling.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lineinfo -g -o ./src/04_1d_block_tiling ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - 1D Block tiling:\n",
      "CUDA kernel time: 65.4218 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1188 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - 1D Block tiling:\n",
      "CUDA kernel time: 65.0267 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1993 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - 1D Block tiling:\n",
      "CUDA kernel time: 65.0944 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1495 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - 1D Block tiling:\n",
      "CUDA kernel time: 65.2405 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - 1D Block tiling:\n",
      "CUDA kernel time: 65.4524 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1572 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - 1D Block tiling:\n",
      "CUDA kernel time: 64.9970 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 65.19 ms\n",
      "Std Dev: 0.15 ms\n",
      "Min: 65.00 ms\n",
      "Max: 65.45 ms\n",
      "Average 1D Block-tiled GEMM time: 65.18576\n"
     ]
    }
   ],
   "source": [
    "block_tile1d_gemm_time = run_gemm(\"./src/04_1d_block_tiling\", 4, 4096, 4096, 4096)\n",
    "print(f\"Average 1D Block-tiled GEMM time: {block_tile1d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 33623 (/home/darshith/code/cuda-gemm-optimization/src/04_1d_block_tiling)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "1D Block tiling:\n",
      "==PROF== Profiling \"gemm1dBlockTiling\" - 1: 0%....50%....100% - 38 passes\n",
      "CUDA kernel time: 14206.7979 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 33623\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/04_1d_block_tiling.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/04_1d_block_tiling ./src/04_1d_block_tiling 4 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_tiling_sol.png\" alt=\"1D tiling SOL\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 1D tiling implementation achieves ~80% SOL for both compute and memory utilization, compared to the coalesced global memory version which had ~85% SOL. The slight decrease suggests room for further optimization in the tiling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_tiling_roofline.png\" alt=\"1D Tiling Roofline\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1D tiling implementation achieves significantly higher performance at 1,603.9 GFLOP/s compared to the shared memory version's 505.0 GFLOP/s, with nearly doubled arithmetic intensity (28.46 vs 15.85 FLOP/byte). This major improvement suggests much better memory reuse through the tiling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 - 2D Block-Tiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading memory:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/stided_memory_load.png\" alt=\"Strided Memory Load\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2D Tiling**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/2d_block_tiling.png\" alt=\"2D Block tiling\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustration demonstrates 2D tiling in the GEMM kernel, where the computation is partitioned into rectangular tiles. Each block of threads works on a submatrix of C (output matrix) of size BM × BN, while shared memory tiles A and B are loaded as BM × BK and BK × BN, respectively.\n",
    "\n",
    "Within each tile, threads compute a grid of results (TM × TN), leveraging shared memory for efficient data reuse during multiple dot-product calculations. This approach minimizes global memory accesses and ensures that each thread contributes to a subgrid of C, enhancing computational efficiency and performance through better locality and parallelism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/05_2d_block_tiling.cu](./src/05_2d_block_tiling.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc -lineinfo -g -o ./src/05_2d_block_tiling ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - 2D Block tiling:\n",
      "CUDA kernel time: 335.4627 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - 2D Block tiling:\n",
      "CUDA kernel time: 337.3605 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - 2D Block tiling:\n",
      "CUDA kernel time: 353.7322 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - 2D Block tiling:\n",
      "CUDA kernel time: 358.5679 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - 2D Block tiling:\n",
      "CUDA kernel time: 353.0319 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - 2D Block tiling:\n",
      "CUDA kernel time: 339.3357 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - 2D Block tiling:\n",
      "CUDA kernel time: 332.3945 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - 2D Block tiling:\n",
      "CUDA kernel time: 327.5722 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - 2D Block tiling:\n",
      "CUDA kernel time: 353.7133 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - 2D Block tiling:\n",
      "CUDA kernel time: 366.4339 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 345.76 ms\n",
      "Std Dev: 12.90 ms\n",
      "Min: 327.57 ms\n",
      "Max: 366.43 ms\n",
      "Average 2D Block-tiled GEMM time: 345.76048\n"
     ]
    }
   ],
   "source": [
    "block_tile2d_gemm_time = run_gemm(\"./src/05_2d_block_tiling\", 5, 4096, 4096, 4096)\n",
    "print(f\"Average 2D Block-tiled GEMM time: {block_tile2d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 33992 (/home/darshith/code/cuda-gemm-optimization/src/05_2d_block_tiling)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "2D Block tiling:\n",
      "==PROF== Profiling \"gemm2dBlockTiling\" - 1: 0%....50%....100% - 38 passes\n",
      "CUDA kernel time: 49679.2695 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 33992\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/05_2d_block_tiling.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/05_2d_block_tiling ./src/05_2d_block_tiling 5 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/2d_tiling_sol.png\" alt=\"2D tiling SOL\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D tiling's poor performance (15% compute, 55% memory SOL) stems from unaligned global memory access patterns. Unlike 1D tiling's aligned access achieving 80% SOL, misaligned memory transactions in 2D tiling cause memory bandwidth underutilization and thread warp inefficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/2d_tiling_roofline.png\" alt=\"2D Tiling Roofline\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D tiling shows significantly lower performance at 248.7 GFLOP/s versus 1D tiling's 1,603.9 GFLOP/s, with much lower arithmetic intensity (4.76 vs 28.46 FLOP/byte). This stark difference reflects the impact of unaligned memory accesses in the 2D implementation compared to 1D's optimized memory patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6 - Vectorized 2D Block-Tiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/vectorized_2d_block_tiling.png\" alt=\"Vectorized 2D Block tiling\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing the As matrix transforms the memory access pattern from vertical (strided) to horizontal (contiguous), enabling better memory coalescing. When threads in a warp access consecutive memory locations after transposition, rather than strided locations in the original format, the GPU can fetch data in fewer memory transactions. This is depicted in the diagram where the dotted line shows \"Now As matrix can be vectorized as well,\" illustrating how the transposed layout allows for efficient vectorized memory access patterns, ultimately improving memory bandwidth utilization and kernel performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/06_vectorize_gemm.cu](./src/06_vectorize_gemm.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lineinfo -g -o ./src/06_vectorize_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.7209 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.9169 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.6604 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.9321 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.8163 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 24.1798 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.9954 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 24.5448 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 24.3349 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 24.0509 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 24.02 ms\n",
      "Std Dev: 0.27 ms\n",
      "Min: 23.66 ms\n",
      "Max: 24.54 ms\n",
      "Average 2D Block-tiled GEMM time: 24.01524\n"
     ]
    }
   ],
   "source": [
    "vector_block_tile2d_gemm_time = run_gemm(\"./src/06_vectorize_gemm\", 6, 4096, 4096, 4096)\n",
    "print(f\"Average 2D Block-tiled GEMM time: {vector_block_tile2d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 34447 (/home/darshith/code/cuda-gemm-optimization/src/06_vectorize_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "Vector - 2D Block tiling:\n",
      "==PROF== Profiling \"gemmVec2dBlockTiling\" - 1: 0%....50%....100% - 38 passes\n",
      "CUDA kernel time: 4075.8435 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 34447\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/06_vectorize_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/06_vectorize_gemm ./src/06_vectorize_gemm 6 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/vector_2d_tiling_sol.png\" alt=\"Vector 2D tiling SOL\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimized 2D tiling implementation shows significantly higher utilization with ~70% compute and ~85% memory SOL, compared to the previous 2D tiling's poor 15% compute and 55% memory utilization. The improvement comes from vectorized memory operations that ensure aligned memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/vector_2d_tiling_roofline.png\" alt=\"Vector 2D Tiling Roofline\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorized 2D tiling achieves 3,824.1 GFLOP/s with 58.25 FLOP/byte intensity, a dramatic improvement over the basic 2D tiling's 248.7 GFLOP/s and 4.76 FLOP/byte. This 15x performance gain comes from properly aligned vectorized memory access patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
