{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **General Matrix Multiplication (GEMM) Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-requistes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code adds the CUDA 12.6 compiler's location to the system's PATH environment variable and then displays the version information for NVIDIA's CUDA compiler (nvcc), which is used for GPU programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda-12.6/bin\" # Add your path to CUDA\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Device details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  7 02:53:26 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.59                 Driver Version: 556.13         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8             12W /   95W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function to execute object files repeatedly and print the average kernel runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import statistics\n",
    "import time  # Added for sleep functionality\n",
    "\n",
    "def run_gemm(executable_path, choice, m, n, k, runs=10, sleep_time=2):  # Added sleep_time parameter\n",
    "    times = []\n",
    "    \n",
    "    for i in range(runs):\n",
    "        result = subprocess.run([executable_path, str(choice), str(m), str(n), str(k)],\n",
    "                              capture_output=True, text=True)\n",
    "        print(f\"Output {i+1} - {result.stdout}\") \n",
    "        match = re.search(r'CUDA kernel time: (\\d+\\.\\d+)', result.stdout)\n",
    "        if match:\n",
    "            cuda_time = float(match.group(1))\n",
    "            times.append(cuda_time)\n",
    "        else:\n",
    "            print(f\"Warning: No time found in output: {result.stdout}\")\n",
    "        \n",
    "        # Add sleep between runs, except for the last run\n",
    "        if i < runs - 1:\n",
    "            time.sleep(sleep_time)\n",
    "            print(f\"Sleeping for {sleep_time} seconds...\")\n",
    "    \n",
    "    # Calculate and print statistics\n",
    "    mean_time = statistics.mean(times)\n",
    "    std_dev = statistics.stdev(times) if len(times) > 1 else 0\n",
    "    min_time = min(times) if times else 0\n",
    "    max_time = max(times) if times else 0\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Mean: {mean_time:.2f} ms\")\n",
    "    print(f\"Std Dev: {std_dev:.2f} ms\")\n",
    "    print(f\"Min: {min_time:.2f} ms\")\n",
    "    print(f\"Max: {max_time:.2f} ms\")\n",
    "    \n",
    "    return mean_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 - Naive GEMM Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_kernel_mul.png\" alt=\"Naive GEMM Multiplication\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows a naive GEMM (General Matrix Multiplication) kernel implementation using threads. Each thread accesses matrix elements based on its ID: `x = blockDim.x * blockIdx.x + threadIdx.x` and `y = blockDim.y * blockIdx.y + threadIdx.y`. Within the B matrix, threads in a warp access the same values (broadcast), while in the A matrix, threads access non-consecutive memory locations (non-coalesced memory access), which is inefficient. The C matrix shows how different threads (0,0), (0,1), (0,2) etc., compute their respective output elements through these memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_kernel_memory_access.png\" alt=\"Naive Kernel Memory Access\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates a memory access pattern issue in GPU computing. It shows two warps (groups of threads) accessing memory in a non-coalesced pattern, meaning threads access scattered memory locations rather than consecutive ones. Each warp requires 4x32B loads (8 loads total), which is inefficient. The crossing lines between thread indices and memory locations visualize this scattered access pattern. This non-optimal memory access results in performance penalties because too many separate load operations are needed to execute each warp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/01_naive_gemm.cu](./src/01_naive_gemm.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc -lineinfo -g -o ./src/01_naive_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1226.8019 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1237.8530 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1254.0999 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1255.6365 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1215.9648 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1230.0734 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1246.9884 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1255.2172 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1256.9995 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1220.3922 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 1240.00 ms\n",
      "Std Dev: 15.82 ms\n",
      "Min: 1215.96 ms\n",
      "Max: 1257.00 ms\n",
      "Average Naive GEMM time: 1240.00268\n"
     ]
    }
   ],
   "source": [
    "naive_gemm_time = run_gemm(\"./src/01_naive_gemm\", 1, 4096, 4096, 4096)\n",
    "print(f\"Average Naive GEMM time: {naive_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 31766 (/home/darshith/code/cuda-gemm-optimization/src/01_naive_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "Naive GEMM Kernel:\n",
      "==PROF== Profiling \"gemmNaive\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "CUDA kernel time: 190328.3906 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 31766\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/01_naive_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "! ncu -f --set full -o ./profiles/01_naive_gemm ./src/01_naive_gemm 1 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_sol.png\" alt=\"Naive SOL\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU throughput graph shows significant memory utilization (approximately 90% of SOL) while compute utilization is relatively low (around 15% of SOL). This indicates the naive GEMM implementation is heavily memory-bound, primarily constrained by memory bandwidth rather than computational capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_roofline.png\" alt=\"Naive Roofline\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The roofline plot reveals this naive GEMM implementation is significantly underperforming compared to the hardware's theoretical peak performance, achieving around 51.8 TFLOP/s with an arithmetic intensity of 14.58 FLOP/byte. The large gap between the actual performance point and the peak performance line suggests substantial room for optimization through techniques like better memory access patterns, cache utilization, and vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Workload Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_memory_workload_analysis.png\" alt=\"Naive Memory Workload Analysis\" width=\"2000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This GEMM kernel shows suboptimal memory access patterns. The L1TEX hit rate is high at 99.11%, but the memory throughput is quite low at 3.56 GB/s, suggesting memory bandwidth isn't fully utilized. Both global load and store access patterns are inefficient, with only 4.0 bytes out of 32 bytes per sector being utilized per thread (indicated by the 86.62% speedup estimate). This points to non-coalesced memory accesses and potential thread stride issues that are preventing optimal memory bandwidth utilization, despite good cache performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 - Coalesced Memory GEMM Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/coalesced_memory_mul.png\" alt=\"Coalesced Memory Multiplication\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows an optimized memory coalesced GEMM (General Matrix Multiplication) kernel design. Unlike the naive version, threads access consecutive memory locations in matrix B, enabling memory coalescing and better performance. For matrix A, all threads within a warp access the same values (broadcast). The coordinates are calculated as: `x = blockIdx.x * BLOCK_SIZE + (threadIdx.x / BLOCK_SIZE)` for matrix A's row access, and `y = blockIdx.y * BLOCK_SIZE + (threadIdx.y % BLOCK_SIZE)` for matrix B's column access. Threads (0,0), (0,1), and (0,2) are grouped in the same warp to optimize memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/coalesced_memory_access.png\" alt=\"Coalesced Memory Access\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows an optimized memory coalesced access pattern where threads within each warp (Warp-0 and Warp-1) access consecutive memory locations. Each warp now requires only 2x32B loads (4 loads total), half of what was needed in the non-coalesced version. The straight vertical lines from thread indices to memory locations indicate efficient coalesced memory access, improving performance by reducing the number of required load operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/02_memory_coalesced_gemm.cu](./src/02_memory_coalesced_gemm.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lineinfo -g -o ./src/02_memory_coalesced_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Global Memory Coalescing:\n",
      "CUDA kernel time: 245.2312 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Global Memory Coalescing:\n",
      "CUDA kernel time: 242.1448 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Global Memory Coalescing:\n",
      "CUDA kernel time: 243.8303 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Global Memory Coalescing:\n",
      "CUDA kernel time: 248.2520 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Global Memory Coalescing:\n",
      "CUDA kernel time: 239.8353 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Global Memory Coalescing:\n",
      "CUDA kernel time: 246.4755 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Global Memory Coalescing:\n",
      "CUDA kernel time: 245.2530 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Global Memory Coalescing:\n",
      "CUDA kernel time: 240.7190 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Global Memory Coalescing:\n",
      "CUDA kernel time: 245.7898 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Global Memory Coalescing:\n",
      "CUDA kernel time: 251.1602 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 244.87 ms\n",
      "Std Dev: 3.43 ms\n",
      "Min: 239.84 ms\n",
      "Max: 251.16 ms\n",
      "Average Global Memory Coalesced GEMM time: 244.86911\n"
     ]
    }
   ],
   "source": [
    "memory_coalesced_gemm_time = run_gemm(\"./src/02_memory_coalesced_gemm\", 2, 4096, 4096, 4096)\n",
    "print(f\"Average Global Memory Coalesced GEMM time: {memory_coalesced_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 32591 (/home/darshith/code/cuda-gemm-optimization/src/02_memory_coalesced_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "Global Memory Coalescing:\n",
      "==PROF== Profiling \"gemmMemCoalesced\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "CUDA kernel time: 104711.6641 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 32591\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/02_memory_coalesced_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/02_memory_coalesced_gemm ./src/02_memory_coalesced_gemm 2 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/gmem_coalesce_sol.png\" alt=\"Global Memory Coalesce SOL\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the naive version's heavily memory-bound performance (~90% memory, ~15% compute SOL), this optimized implementation shows balanced utilization with both compute and memory throughput reaching around 85% SOL. This balance, achieved through memory coalescing, indicates more efficient use of both computational and memory resources on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/gmem_coalesce_roofline.png\" alt=\"Global Memory Coalesce Roofline\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the naive implementation (51.8 GFLOP/s), this optimized version achieves 374.9 GFLOP/s with a slightly higher arithmetic intensity of 15.14 FLOP/byte, demonstrating a 7.2x speedup primarily through global memory coalescing, where adjacent threads access contiguous memory addresses to combine memory transactions into fewer, larger operations, significantly reducing memory latency and increasing effective bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Workload Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/gmem_coalesce_mwa.png\" alt=\"Global Memory Coalesce - Memory Workload Analysis\" width=\"2000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The coalesced version shows significant improvements in memory efficiency. Memory throughput increased from 3.56 GB/s to 24.80 GB/s (~7x improvement), and memory utilization per sector improved from 4.0 to 26.4 bytes out of 32 bytes per thread. Memory pipe utilization also increased from 12% to 86.91%. While both kernels maintain high L1TEX hit rates (>94%), the coalesced version's better memory access patterns result in substantially higher bandwidth utilization and overall memory throughput, demonstrating the importance of proper memory coalescing in GEMM implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/gmem_coalesce_memory_chart.png\" alt=\"Global Memory Coalesce - Memory Chart\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows 4.30G memory transactions being fetched from global memory. Memory access latencies: global memory takes 400-800 cycles while shared memory only takes 20-30 cycles. Implementing shared memory would significantly improve performance by reducing these high-latency global memory accesses, especially for data that's frequently reused within thread blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 -  Shared Memory Cache-Blocking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/shared_memory_cache_blocking.png\" alt=\"Shared Memory Cache Blocking\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates a block-based matrix multiplication algorithm with a block size of 32. When multiplying matrices A and C, each matrix is divided into blocks of size BLOCK_SIZE (32). The starting addresses of blocks are calculated using formulas: `&A = row * BLOCK_SIZE * K` for matrix A's rows, `&B = col * BLOCK_SIZE` for B's columns, and `&C = (row * BLOCK_SIZE * K) + (col * BLOCK_SIZE)` for matrix C's position. As the algorithm processes each block, it increments A by BLOCK_SIZE within the same row, B by BLOCK_SIZE * N to move to the next block, and C moves to process the next row of blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/03_shared_memory_gemm.cu](./src/03_shared_memory_gemm.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lineinfo -g -o ./src/03_shared_memory_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 161.3495 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 174.2664 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 177.5903 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 178.5169 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 175.0734 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 164.2657 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 163.7582 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 178.3736 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 169.3051 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 177.4289 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 171.99 ms\n",
      "Std Dev: 6.72 ms\n",
      "Min: 161.35 ms\n",
      "Max: 178.52 ms\n",
      "Average Shared Memory GEMM time: 171.9928\n"
     ]
    }
   ],
   "source": [
    "shared_memory_gemm_time = run_gemm(\"./src/03_shared_memory_gemm\", 3, 4096, 4096, 4096)\n",
    "print(f\"Average Shared Memory GEMM time: {shared_memory_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 33179 (/home/darshith/code/cuda-gemm-optimization/src/03_shared_memory_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "Shared Memory Cache-Blocking:\n",
      "==PROF== Profiling \"gemmSharedMem\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "CUDA kernel time: 48190.5156 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 33179\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/03_shared_memory_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/03_shared_memory_gemm ./src/03_shared_memory_gemm 3 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/shared_mem_sol.png\" alt=\"Shared Memory SOL\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shared memory version shows slightly lower GPU throughput (~80% SOL) compared to the coalesced version (~85% SOL), but still maintains balanced compute and memory utilization. It leverages faster shared memory access to reduce global memory traffic, though the overhead of shared memory operations may explain the small performance difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/shared_mem_roofline.png\" alt=\"Shared Memory Roofline\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shared memory implementation reaches 505.0 GFLOP/s compared to 374.9 GFLOP/s in the previous version, with a slightly higher arithmetic intensity (15.85 vs 15.14 FLOP/byte). The 130.1 GFLOP/s improvement comes from optimized tile sizes and shared memory access patterns while maintaining ~80% SOL utilization. Arithmetic Intensity can be increased by having each thread perform operations on multiple locations of output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Workload Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/smem_block_tiling_memory_chart.png\" alt=\"Shared Memory Roofline\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two memory charts:\n",
    "\n",
    "Global Memory Only:\n",
    "- 4.30G memory transactions through global memory\n",
    "- No shared memory utilization\n",
    "\n",
    "Shared Memory with Block Tiling:\n",
    "- Global memory transactions reduced to 268.44M (~ 16x reduction)\n",
    "- 2.82G transactions now using shared memory\n",
    "- Shared memory's 20-30 cycle latency vs global memory's 400-800 cycles means significant performance improvement\n",
    "- More efficient data reuse through shared memory block tiling, reducing high-latency global memory accesses\n",
    "\n",
    "The dramatic reduction in global memory transactions and shift to faster shared memory access demonstrates the performance benefits of block tiling implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing memory access latencies across the hierarchy:\n",
    "- Registers: ~1 cycle\n",
    "- Shared Memory: 20-30 cycles \n",
    "- Global Memory: 400-800 cycles\n",
    "\n",
    "The current block tiling implementation shows good performance by moving from global to shared memory. Further optimization using register blocking would improve performance even more by leveraging single-cycle register access latency compared to shared memory's 20-30 cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 - 1D Block-Tiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_block_tiling.png\" alt=\"1D Block tiling\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `TM` in the kernel specifies the number of rows in the output tile of \\( C \\) that each thread computes. Instead of handling a single element, each thread processes \\( TM \\) rows for its assigned column, storing partial results in local registers (`threadResult[TM]`). This approach increases the amount of computation per thread, reducing idle cycles and improving efficiency. By reusing the same tiles of \\( A \\) and \\( B \\) loaded into shared memory to compute all \\( TM \\) rows, the kernel minimizes global memory traffic, enhancing memory locality. This design choice significantly boosts arithmetic intensity by performing more computations relative to the data moved from global memory, leading to better utilization of GPU resources and improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/04_1d_block_tiling.cu](./src/04_1d_block_tiling.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lineinfo -g -o ./src/04_1d_block_tiling ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - 1D Block tiling:\n",
      "CUDA kernel time: 65.4218 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1188 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - 1D Block tiling:\n",
      "CUDA kernel time: 65.0267 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1993 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - 1D Block tiling:\n",
      "CUDA kernel time: 65.0944 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1495 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - 1D Block tiling:\n",
      "CUDA kernel time: 65.2405 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - 1D Block tiling:\n",
      "CUDA kernel time: 65.4524 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1572 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - 1D Block tiling:\n",
      "CUDA kernel time: 64.9970 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 65.19 ms\n",
      "Std Dev: 0.15 ms\n",
      "Min: 65.00 ms\n",
      "Max: 65.45 ms\n",
      "Average 1D Block-tiled GEMM time: 65.18576\n"
     ]
    }
   ],
   "source": [
    "block_tile1d_gemm_time = run_gemm(\"./src/04_1d_block_tiling\", 4, 4096, 4096, 4096)\n",
    "print(f\"Average 1D Block-tiled GEMM time: {block_tile1d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 33623 (/home/darshith/code/cuda-gemm-optimization/src/04_1d_block_tiling)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "1D Block tiling:\n",
      "==PROF== Profiling \"gemm1dBlockTiling\" - 1: 0%....50%....100% - 38 passes\n",
      "CUDA kernel time: 14206.7979 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 33623\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/04_1d_block_tiling.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/04_1d_block_tiling ./src/04_1d_block_tiling 4 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_tiling_sol.png\" alt=\"1D tiling SOL\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 1D tiling implementation achieves ~80% SOL for both compute and memory utilization, compared to the coalesced global memory version which had ~85% SOL. The slight decrease suggests room for further optimization in the tiling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_tiling_roofline.png\" alt=\"1D Tiling Roofline\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1D tiling implementation achieves significantly higher performance at 1,603.9 GFLOP/s compared to the shared memory version's 505.0 GFLOP/s, with nearly doubled arithmetic intensity (28.46 vs 15.85 FLOP/byte). This major improvement suggests much better memory reuse through the tiling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Workload Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_tiling_mwa.png\" alt=\"1D Tiling - Memory Work Loan\" width=\"2000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This memory workload analysis shows:\n",
    "- Memory throughput: 56.09 GB/s\n",
    "- Very low L1TEX hit rate (0.78%), indicating minimal cache reuse\n",
    "- High memory pipe utilization (81.30%)\n",
    "- High memory bus utilization (69.07% Mem Busy)\n",
    "\n",
    "The low cache hit rate but high throughput suggests this kernel is effectively streaming data through memory without relying on cache, which is expected for a 1D tiled GEMM implementation utilizing register-level data reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_tiling_memory_chart.png\" alt=\"1D Tiling Memory Chart\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the memory chart, shared memory transactions reduced from 2.82G to 872.42M in this 1D tiled implementation because:\n",
    "1. Matrix B's values are stored in registers for reuse\n",
    "2. Matrix C uses register accumulation\n",
    "3. Each thread computes multiple outputs, increasing data reuse since the same input values can be used for multiple computations before needing new data from shared memory\n",
    "\n",
    "This thread-level data reuse through registers significantly reduces the frequency of shared memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 - 2D Block-Tiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading memory:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/stided_memory_load.png\" alt=\"Strided Memory Load\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2D Tiling**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/2d_block_tiling.png\" alt=\"2D Block tiling\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustration demonstrates 2D tiling in the GEMM kernel, where the computation is partitioned into rectangular tiles. Each block of threads works on a submatrix of C (output matrix) of size BM × BN, while shared memory tiles A and B are loaded as BM × BK and BK × BN, respectively.\n",
    "\n",
    "Within each tile, threads compute a grid of results (TM × TN), leveraging shared memory for efficient data reuse during multiple dot-product calculations. This approach minimizes global memory accesses and ensures that each thread contributes to a subgrid of C, enhancing computational efficiency and performance through better locality and parallelism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/05_2d_block_tiling.cu](./src/05_2d_block_tiling.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc -lineinfo -g -o ./src/05_2d_block_tiling ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - 2D Block tiling:\n",
      "CUDA kernel time: 335.4627 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - 2D Block tiling:\n",
      "CUDA kernel time: 337.3605 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - 2D Block tiling:\n",
      "CUDA kernel time: 353.7322 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - 2D Block tiling:\n",
      "CUDA kernel time: 358.5679 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - 2D Block tiling:\n",
      "CUDA kernel time: 353.0319 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - 2D Block tiling:\n",
      "CUDA kernel time: 339.3357 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - 2D Block tiling:\n",
      "CUDA kernel time: 332.3945 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - 2D Block tiling:\n",
      "CUDA kernel time: 327.5722 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - 2D Block tiling:\n",
      "CUDA kernel time: 353.7133 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - 2D Block tiling:\n",
      "CUDA kernel time: 366.4339 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 345.76 ms\n",
      "Std Dev: 12.90 ms\n",
      "Min: 327.57 ms\n",
      "Max: 366.43 ms\n",
      "Average 2D Block-tiled GEMM time: 345.76048\n"
     ]
    }
   ],
   "source": [
    "block_tile2d_gemm_time = run_gemm(\"./src/05_2d_block_tiling\", 5, 4096, 4096, 4096)\n",
    "print(f\"Average 2D Block-tiled GEMM time: {block_tile2d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 33992 (/home/darshith/code/cuda-gemm-optimization/src/05_2d_block_tiling)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "2D Block tiling:\n",
      "==PROF== Profiling \"gemm2dBlockTiling\" - 1: 0%....50%....100% - 38 passes\n",
      "CUDA kernel time: 49679.2695 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 33992\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/05_2d_block_tiling.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/05_2d_block_tiling ./src/05_2d_block_tiling 5 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/2d_tiling_sol.png\" alt=\"2D tiling SOL\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D tiling's poor performance (15% compute, 55% memory SOL) stems from unaligned global memory access patterns. Unlike 1D tiling's aligned access achieving 80% SOL, misaligned memory transactions in 2D tiling cause memory bandwidth underutilization and thread warp inefficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/2d_tiling_roofline.png\" alt=\"2D Tiling Roofline\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D tiling shows significantly lower performance at 248.7 GFLOP/s versus 1D tiling's 1,603.9 GFLOP/s, with much lower arithmetic intensity (4.76 vs 28.46 FLOP/byte). This stark difference reflects the impact of unaligned memory accesses in the 2D implementation compared to 1D's optimized memory patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Workload Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/2d_tiling_mwa.png\" alt=\"2D Tiling Memory Workload Analysis\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing 1D vs 2D tiling memory analysis:\n",
    "\n",
    "1D Tiling:\n",
    "- Memory throughput: 56.09 GB/s\n",
    "- Very low L1TEX hit rate (0.78%)\n",
    "- High memory pipe utilization (81.30%)\n",
    "\n",
    "2D Tiling:\n",
    "- Memory throughput: 52.09 GB/s\n",
    "- Much higher L1TEX hit rate (95.46%)\n",
    "- Lower memory pipe utilization (19.60%)\n",
    "- Shows significant bank conflicts in shared memory:\n",
    "  - 3.2-way bank conflicts for loads (50% of wavefronts)\n",
    "  - 1.2-way bank conflicts for stores (14.05% of wavefronts)\n",
    "\n",
    "2D tiling trades slightly lower throughput for better cache utilization, but introduces shared memory bank conflicts that weren't present in 1D tiling. These bank conflicts could be limiting the potential performance gains from 2D tiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/2d_tiling_memory_chart.png\" alt=\"2D Tiling Memory Chart\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing 1D vs 2D tiling memory charts:\n",
    "\n",
    "1D Tiling:\n",
    "- Global: 68.16M transactions\n",
    "- Shared: 872.42M transactions\n",
    "\n",
    "2D Tiling:\n",
    "- Global: 570.43M transactions\n",
    "- Shared: 369.10M transactions (↓57% from 1D)\n",
    "- Higher cache hit rates (L1: 95.46%, L2: 98.79%)\n",
    "\n",
    "2D tiling significantly reduces shared memory transactions due to better data reuse through registers, though it shows higher global memory transactions. The improved cache hit rates suggest better memory locality compared to 1D tiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6 - Vectorized 2D Block-Tiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/vectorized_2d_block_tiling.png\" alt=\"Vectorized 2D Block tiling\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing the As matrix transforms the memory access pattern from vertical (strided) to horizontal (contiguous), enabling better memory coalescing. When threads in a warp access consecutive memory locations after transposition, rather than strided locations in the original format, the GPU can fetch data in fewer memory transactions. This is depicted in the diagram where the dotted line shows \"Now As matrix can be vectorized as well,\" illustrating how the transposed layout allows for efficient vectorized memory access patterns, ultimately improving memory bandwidth utilization and kernel performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Compile the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file being executed is [./src/06_vectorize_gemm.cu](./src/06_vectorize_gemm.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -lineinfo -g -o ./src/06_vectorize_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Execute the object file multiple times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.7209 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.9169 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.6604 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.9321 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.8163 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 24.1798 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 23.9954 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 24.5448 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 24.3349 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Vector - 2D Block tiling:\n",
      "CUDA kernel time: 24.0509 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 24.02 ms\n",
      "Std Dev: 0.27 ms\n",
      "Min: 23.66 ms\n",
      "Max: 24.54 ms\n",
      "Average 2D Block-tiled GEMM time: 24.01524\n"
     ]
    }
   ],
   "source": [
    "vector_block_tile2d_gemm_time = run_gemm(\"./src/06_vectorize_gemm\", 6, 4096, 4096, 4096)\n",
    "print(f\"Average 2D Block-tiled GEMM time: {vector_block_tile2d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Generate report for the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 34447 (/home/darshith/code/cuda-gemm-optimization/src/06_vectorize_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "Vector - 2D Block tiling:\n",
      "==PROF== Profiling \"gemmVec2dBlockTiling\" - 1: 0%....50%....100% - 38 passes\n",
      "CUDA kernel time: 4075.8435 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 34447\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/06_vectorize_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu -f --set full -o ./profiles/06_vectorize_gemm ./src/06_vectorize_gemm 6 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed of Light Throughput:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/vector_2d_tiling_sol.png\" alt=\"Vector 2D tiling SOL\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimized 2D tiling implementation shows significantly higher utilization with ~70% compute and ~85% memory SOL, compared to the previous 2D tiling's poor 15% compute and 55% memory utilization. The improvement comes from vectorized memory operations that ensure aligned memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roofline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/vector_2d_tiling_roofline.png\" alt=\"Vector 2D Tiling Roofline\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorized 2D tiling achieves 3,824.1 GFLOP/s with 58.25 FLOP/byte intensity, a dramatic improvement over the basic 2D tiling's 248.7 GFLOP/s and 4.76 FLOP/byte. This 15x performance gain comes from properly aligned vectorized memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Workload Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/vector_2d_tiling_mwa.png\" alt=\"Vector 2D Tiling Memory Workload Analysis\" width=\"1500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing vectorized vs regular 2D tiling:\n",
    "\n",
    "- Memory Performance:\n",
    "    - Throughput increased: 52.09 → 65.64 GB/s\n",
    "    - Memory bus utilization improved: 47.13% → 83.55%\n",
    "\n",
    "- Bank Conflicts:\n",
    "    - Previous: 3.2-way load conflicts (50%)\n",
    "    - Current: 5.0-way load conflicts (40%)\n",
    "\n",
    "- Global Memory:\n",
    "    - Previous: 4.4/32 bytes utilized per sector\n",
    "    - Current: 31.5/32 bytes utilized per sector (significant coalescing improvement)\n",
    "\n",
    "The vectorized implementation shows better memory throughput and global memory coalescing, though with slightly higher bank conflicts in shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/vector_2d_tiling_memory_chart.png\" alt=\"Vector 2D Tiling Memory Chart\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing memory charts for vectorized 2D tiling:\n",
    "\n",
    "- Previous:\n",
    "    - Global: 570.43M transactions\n",
    "    - Shared: 369.10M transactions\n",
    "\n",
    "- Current:\n",
    "    - Global: 8.65M transactions (↓98%)\n",
    "    - Shared: 155.19M transactions (↓58%)\n",
    "    - Significant reduction in total memory operations due to vectorization's efficient data loading and register usage\n",
    "\n",
    "Both L1 and L2 cache hit rates remain similar, but the dramatic reduction in memory transactions shows vectorization's effectiveness at memory access optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
