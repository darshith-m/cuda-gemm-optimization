{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **General Matrix Multiplication (GEMM) Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda-12.6/bin\"\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import statistics\n",
    "import time  # Added for sleep functionality\n",
    "\n",
    "def run_gemm(executable_path, choice, m, n, k, runs=10, sleep_time=2):  # Added sleep_time parameter\n",
    "    times = []\n",
    "    \n",
    "    for i in range(runs):\n",
    "        result = subprocess.run([executable_path, str(choice), str(m), str(n), str(k)],\n",
    "                              capture_output=True, text=True)\n",
    "        print(f\"Output {i+1} - {result.stdout}\") \n",
    "        match = re.search(r'CUDA kernel time: (\\d+\\.\\d+)', result.stdout)\n",
    "        if match:\n",
    "            cuda_time = float(match.group(1))\n",
    "            times.append(cuda_time)\n",
    "        else:\n",
    "            print(f\"Warning: No time found in output: {result.stdout}\")\n",
    "        \n",
    "        # Add sleep between runs, except for the last run\n",
    "        if i < runs - 1:\n",
    "            time.sleep(sleep_time)\n",
    "            print(f\"Sleeping for {sleep_time} seconds...\")\n",
    "    \n",
    "    # Calculate and print statistics\n",
    "    mean_time = statistics.mean(times)\n",
    "    std_dev = statistics.stdev(times) if len(times) > 1 else 0\n",
    "    min_time = min(times) if times else 0\n",
    "    max_time = max(times) if times else 0\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Mean: {mean_time:.2f} ms\")\n",
    "    print(f\"Std Dev: {std_dev:.2f} ms\")\n",
    "    print(f\"Min: {min_time:.2f} ms\")\n",
    "    print(f\"Max: {max_time:.2f} ms\")\n",
    "    \n",
    "    return mean_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 - Naive GEMM Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_kernel_mul.png\" alt=\"Naive GEMM Multiplication\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows a naive GEMM (General Matrix Multiplication) kernel implementation using threads. Each thread accesses matrix elements based on its ID: x = blockDim.x * blockIdx.x + threadIdx.x and y = blockDim.y * blockIdx.y + threadIdx.y. Within the B matrix, threads in a warp access the same values (broadcast), while in the A matrix, threads access non-consecutive memory locations (non-coalesced memory access), which is inefficient. The C matrix shows how different threads (0,0), (0,1), (0,2) etc., compute their respective output elements through these memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/naive_kernel_memory_access.png\" alt=\"Naive Kernel Memory Access\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates a memory access pattern issue in GPU computing. It shows two warps (groups of threads) accessing memory in a non-coalesced pattern, meaning threads access scattered memory locations rather than consecutive ones. Each warp requires 4x32B loads (8 loads total), which is inefficient. The crossing lines between thread indices and memory locations visualize this scattered access pattern. This non-optimal memory access results in performance penalties because too many separate load operations are needed to execute each warp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o ./src/01_naive_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1289.4343 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1286.6808 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1286.7893 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1296.2511 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1268.7273 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1254.4408 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1259.7162 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1257.1493 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1297.1204 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Naive GEMM Kernel:\n",
      "CUDA kernel time: 1289.5819 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 1278.59 ms\n",
      "Std Dev: 16.74 ms\n",
      "Min: 1254.44 ms\n",
      "Max: 1297.12 ms\n",
      "Average Naive GEMM time: 1278.58914\n"
     ]
    }
   ],
   "source": [
    "naive_gemm_time = run_gemm(\"./src/01_naive_gemm\", 1, 4096, 4096, 4096)\n",
    "print(f\"Average Naive GEMM time: {naive_gemm_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 103654 (/home/darshith/code/cuda-gemm-optimization/src/01_naive_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "Naive GEMM Kernel:\n",
      "==PROF== Profiling \"gemmNaive\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "CUDA kernel time: 190976.5625 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 103654\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/01_naive_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu --set full -o ./profiles/01_naive_gemm ./src/01_naive_gemm 1 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 - Coalesced Memory GEMM Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/coalesced_memory_mul.png\" alt=\"Coalesced Memory Multiplication\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows an optimized memory coalesced GEMM (General Matrix Multiplication) kernel design. Unlike the naive version, threads access consecutive memory locations in matrix B, enabling memory coalescing and better performance. For matrix A, all threads within a warp access the same values (broadcast). The coordinates are calculated as: x = blockIdx.x * BLOCK_SIZE + (threadIdx.x / BLOCK_SIZE) for matrix A's row access, and y = blockIdx.y * BLOCK_SIZE + (threadIdx.y % BLOCK_SIZE) for matrix B's column access. Threads (0,0), (0,1), and (0,2) are grouped in the same warp to optimize memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/coalesced_memory_access.png\" alt=\"Coalesced Memory Access\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram shows an optimized memory coalesced access pattern where threads within each warp (Warp-0 and Warp-1) access consecutive memory locations. Each warp now requires only 2x32B loads (4 loads total), half of what was needed in the non-coalesced version. The straight vertical lines from thread indices to memory locations indicate efficient coalesced memory access, improving performance by reducing the number of required load operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o ./src/02_memory_coalesced_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Global Memory Coalescing:\n",
      "CUDA kernel time: 254.7391 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Global Memory Coalescing:\n",
      "CUDA kernel time: 252.1203 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Global Memory Coalescing:\n",
      "CUDA kernel time: 255.5368 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Global Memory Coalescing:\n",
      "CUDA kernel time: 251.8124 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Global Memory Coalescing:\n",
      "CUDA kernel time: 251.9612 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Global Memory Coalescing:\n",
      "CUDA kernel time: 251.5591 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Global Memory Coalescing:\n",
      "CUDA kernel time: 252.3045 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Global Memory Coalescing:\n",
      "CUDA kernel time: 252.6084 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Global Memory Coalescing:\n",
      "CUDA kernel time: 254.9641 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Global Memory Coalescing:\n",
      "CUDA kernel time: 254.6621 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 253.23 ms\n",
      "Std Dev: 1.55 ms\n",
      "Min: 251.56 ms\n",
      "Max: 255.54 ms\n",
      "Average Global Memory Coalesced GEMM time: 253.2268\n"
     ]
    }
   ],
   "source": [
    "memory_coalesced_gemm_time = run_gemm(\"./src/02_memory_coalesced_gemm\", 2, 4096, 4096, 4096)\n",
    "print(f\"Average Global Memory Coalesced GEMM time: {memory_coalesced_gemm_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 104516 (/home/darshith/code/cuda-gemm-optimization/src/02_memory_coalesced_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "Global Memory Coalescing:\n",
      "==PROF== Profiling \"gemmMemCoalesced\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "CUDA kernel time: 105258.9141 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 104516\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/02_memory_coalesced_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu --set full -o ./profiles/02_memory_coalesced_gemm ./src/02_memory_coalesced_gemm 2 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 -  Shared Memory Cache-Blocking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/shared_memory_cache_blocking.png\" alt=\"Shared Memory Cache Blocking\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates a block-based matrix multiplication algorithm with a block size of 32. When multiplying matrices A and C, each matrix is divided into blocks of size BLOCK_SIZE (32). The starting addresses of blocks are calculated using formulas: &A = row * BLOCK_SIZE * K for matrix A's rows, &B = col * BLOCK_SIZE for B's columns, and &C = (row * BLOCK_SIZE * K) + (col * BLOCK_SIZE) for matrix C's position. As the algorithm processes each block, it increments A by BLOCK_SIZE within the same row, B by BLOCK_SIZE * N to move to the next block, and C moves to process the next row of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o ./src/03_shared_memory_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 161.1391 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 182.9159 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 167.4789 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 161.1305 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 182.4557 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 175.5508 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 171.3870 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 179.1420 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 178.5341 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - Shared Memory Cache-Blocking:\n",
      "CUDA kernel time: 182.8965 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 174.26 ms\n",
      "Std Dev: 8.56 ms\n",
      "Min: 161.13 ms\n",
      "Max: 182.92 ms\n",
      "Average Shared Memory GEMM time: 174.26305\n"
     ]
    }
   ],
   "source": [
    "shared_memory_gemm_time = run_gemm(\"./src/03_shared_memory_gemm\", 3, 4096, 4096, 4096)\n",
    "print(f\"Average Shared Memory GEMM time: {shared_memory_gemm_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 105151 (/home/darshith/code/cuda-gemm-optimization/src/03_shared_memory_gemm)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "Shared Memory Cache-Blocking:\n",
      "==PROF== Profiling \"gemmSharedMem\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "CUDA kernel time: 48232.0820 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 105151\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/03_shared_memory_gemm.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu --set full -o ./profiles/03_shared_memory_gemm ./src/03_shared_memory_gemm 3 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 - 1D Block-Tiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/1d_block_tiling.png\" alt=\"1D Block tiling\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o ./src/04_1d_block_tiling ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - 1D Block tiling:\n",
      "CUDA kernel time: 64.9989 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - 1D Block tiling:\n",
      "CUDA kernel time: 65.0620 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - 1D Block tiling:\n",
      "CUDA kernel time: 65.2465 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - 1D Block tiling:\n",
      "CUDA kernel time: 65.4750 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1683 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1523 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - 1D Block tiling:\n",
      "CUDA kernel time: 65.2660 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1588 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - 1D Block tiling:\n",
      "CUDA kernel time: 65.1698 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - 1D Block tiling:\n",
      "CUDA kernel time: 64.9749 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 65.17 ms\n",
      "Std Dev: 0.14 ms\n",
      "Min: 64.97 ms\n",
      "Max: 65.47 ms\n",
      "Average 1D Block-tiled GEMM time: 65.16725\n"
     ]
    }
   ],
   "source": [
    "block_tile1d_gemm_time = run_gemm(\"./src/04_1d_block_tiling\", 4, 4096, 4096, 4096)\n",
    "print(f\"Average 1D Block-tiled GEMM time: {block_tile1d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 105627 (/home/darshith/code/cuda-gemm-optimization/src/04_1d_block_tiling)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "1D Block tiling:\n",
      "==PROF== Profiling \"gemm1dBlockTiling\" - 1: 0%....50%....100% - 37 passes\n",
      "CUDA kernel time: 14103.1299 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 105627\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/04_1d_block_tiling.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu --set full -o ./profiles/04_1d_block_tiling ./src/04_1d_block_tiling 4 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 - 2D Block-Tiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o ./src/05_2d_block_tiling ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 1 - 2D Block tiling:\n",
      "CUDA kernel time: 379.6307 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 2 - 2D Block tiling:\n",
      "CUDA kernel time: 383.0912 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 3 - 2D Block tiling:\n",
      "CUDA kernel time: 370.4710 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 4 - 2D Block tiling:\n",
      "CUDA kernel time: 366.1082 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 5 - 2D Block tiling:\n",
      "CUDA kernel time: 374.2991 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 6 - 2D Block tiling:\n",
      "CUDA kernel time: 371.2763 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 7 - 2D Block tiling:\n",
      "CUDA kernel time: 383.6719 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 8 - 2D Block tiling:\n",
      "CUDA kernel time: 358.5301 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 9 - 2D Block tiling:\n",
      "CUDA kernel time: 372.7195 ms\n",
      "Results match : Yes \n",
      "\n",
      "Sleeping for 2 seconds...\n",
      "Output 10 - 2D Block tiling:\n",
      "CUDA kernel time: 382.7235 ms\n",
      "Results match : Yes \n",
      "\n",
      "\n",
      "Statistics:\n",
      "Mean: 374.25 ms\n",
      "Std Dev: 8.21 ms\n",
      "Min: 358.53 ms\n",
      "Max: 383.67 ms\n",
      "Average 2D Block-tiled GEMM time: 374.25215000000003\n"
     ]
    }
   ],
   "source": [
    "block_tile2d_gemm_time = run_gemm(\"./src/05_2d_block_tiling\", 5, 4096, 4096, 4096)\n",
    "print(f\"Average 2D Block-tiled GEMM time: {block_tile2d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 106033 (/home/darshith/code/cuda-gemm-optimization/src/05_2d_block_tiling)\n",
      "==PROF== Profiling \"ampere_sgemm_128x64_nn\" - 0: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 37 passes\n",
      "2D Block tiling:\n",
      "==PROF== Profiling \"gemm2dBlockTiling\" - 1: 0%\n",
      "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
      "....50%....100% - 38 passes\n",
      "CUDA kernel time: 49358.2031 ms\n",
      "Results match : Yes \n",
      "==PROF== Disconnected from process 106033\n",
      "==PROF== Report: /home/darshith/code/cuda-gemm-optimization/./profiles/05_2d_block_tiling.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "!ncu --set full -o ./profiles/05_2d_block_tiling ./src/05_2d_block_tiling 5 4096 4096 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6 - Vectorized 2D Block-Tiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o ./src/06_vectorize_gemm ./src/run.cu -lcublas -lnvToolsExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_block_tile2d_gemm_time = run_gemm(\"./src/06_vectorize_gemm\", 6, 4096, 4096, 4096)\n",
    "print(f\"Average 2D Block-tiled GEMM time: {vector_block_tile2d_gemm_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncu --set full -o ./profiles/06_vectorize_gemm ./src/06_vectorize_gemm 6 4096 4096 4096"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
